{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into words or sentences and encoding them into a numerical format.\n",
    "It is a crucial step in natural language processing (NLP) and is used in various applications such as text classification, sentiment analysis, and machine translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'today': 2, 'is': 3, 'a': 4, 'sunny': 5, 'day': 6, 'rainy': 7, 'it': 8}\n",
      "[[2, 3, 4, 5, 6], [2, 3, 4, 7, 6], [3, 8, 5, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer example\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\"Today is a sunny day\", \"Today is a rainy day\", \"Is it sunny today?\"]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Print the word index (token dictionary)\n",
    "print(word_index)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Print the sequences\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-vocabulary token\n",
    "\n",
    "An out-of-vocabulary (OOV) token is a special token used to represent words that are not present in the vocabulary of a language model.\n",
    "\n",
    "When a word is not present in the vocabulary, it is replaced with the OOV token to ensure that the model can still process the input text.\n",
    "\n",
    "Add `oov_token=\"<OOV>\"` to the Tokenizer constructor.\n",
    "\n",
    "This will get you from:\n",
    "\"Today is a snowy day\" -> \"today is a day\"\n",
    "\n",
    "To:\n",
    "\"Today is a snowy day\" -> \"today is a <OOV> day\"\n",
    "\n",
    "This detail improves the accuracy of the encoding, preserving context information by keeping the words in thir original position in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'today': 2, 'is': 3, 'a': 4, 'sunny': 5, 'day': 6, 'rainy': 7, 'it': 8}\n",
      "[[2, 3, 4, 1, 6], [1, 8, 1, 7, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Out-of-vocabulary token\n",
    "\n",
    "test_data = [\"Today is a snowy day\", \"Will it be rainy tomorrow?\"]\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "print(word_index)\n",
    "print(test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "As the sentences can vary in length and the model expects a fixed-size input, we need to pad the sequences to ensure that they all have the same length.\n",
    "\n",
    "Add `padding=\"post\"` to the Tokenizer constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'today': 2, 'is': 3, 'a': 4, 'sunny': 5, 'day': 6, 'rainy': 7, 'it': 8}\n",
      "[[2, 3, 4, 5, 6], [2, 3, 4, 7, 6], [3, 8, 5, 2], [1, 1, 1, 1, 1, 1, 1, 2]]\n",
      "[[2 3 4 5 6 0]\n",
      " [2 3 4 7 6 0]\n",
      " [3 8 5 2 0 0]\n",
      " [1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    \"Today is a sunny day\",\n",
    "    \"Today is a rainy day\",\n",
    "    \"Is it sunny today?\",\n",
    "    \"I really enjoyed walking in the snow today\",\n",
    "]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "\n",
    "padded = pad_sequences(sequences, padding=\"post\", maxlen=6, truncating=\"post\")\n",
    "\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Pre-processing Optimizations for Tokenization\n",
    "\n",
    "- Lowercasing: Convert all text to lowercase to reduce the vocabulary size and improve generalization.\n",
    "- Removing punctuation: Remove punctuation marks from the text to reduce the vocabulary size and improve generalization.\n",
    "- Removing stop words: Remove common words such as \"the\", \"is\", and \"and\" that do not carry much meaning to reduce the vocabulary size and improve generalization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
